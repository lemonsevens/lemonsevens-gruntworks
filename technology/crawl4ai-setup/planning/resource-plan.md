# Resource Plan Documentation for Web Crawl for AI Setup

## Core Resource(s)
- **crawl4ai**: v0.5.0
- **Docker Engine**: Community Edition (latest stable CE)

## Supporting Resources

### Infrastructure
- **Operating System**: Ubuntu LTS (latest stable)
  - **Purpose**: Host OS for the DigitalOcean droplet running Docker.
  - **Chosen because**: Common, stable, well-supported Linux distribution for Docker and DigitalOcean, extensive documentation and community support.

### Configuration Management
- **Configuration Files**: YAML or JSON
  - **Purpose**: Define extraction schemas and potentially crawler configurations for `crawl4ai` scripts.
  - **Chosen because**: Human-readable, standard formats easily parsed by Python, manageable in version control.
- **Version Control**: Git (Existing Project Repository)
  - **Purpose**: Manage configuration files, schemas, custom scripts, and deployment definitions (e.g., Dockerfile, docker-compose.yml).
  - **Chosen because**: Standard practice for tracking changes, collaboration, and deployment management.

### Data Handling - Database
- **Database System**: PostgreSQL (latest stable)
  - **Purpose**: Target database for direct export of extracted data (REQ-FR-DAT-4).
  - **Chosen because**: Robust, open-source, widely used SQL database suitable for structured data; good Python support.
- **Deployment**: Run as a separate Docker container.

### Data Handling - Files
- **File Formats**: CSV, JSON
  - **Purpose**: Export formats for extracted data (REQ-FR-DAT-2, REQ-FR-DAT-3).
  - **Chosen because**: Standard, easily parsed formats for data interchange, likely natively handled or easily generated by Python scripts using `crawl4ai`.
- **Storage**: Docker Volumes or Host File System Mounts
  - **Purpose**: Persistent storage for exported files and potentially database data.
  - **Chosen because**: Standard Docker practice for ensuring data persistence beyond container lifecycles.

### Access Control
- **Method**: SSH Access Control (to the DigitalOcean droplet)
  - **Purpose**: Restrict execution/management access to the internal team (REQ-FR-ACC-1, REQ-FR-ACC-2).
  - **Chosen because**: Simple, secure, and standard method for server access for internal tools. Further access controls (e.g., firewall rules, VPN, basic auth for web wrappers) can be layered if needed.

### Integration - Webhooks
- **Handling Method**: External Automation Tool (e.g., N8N) or dedicated lightweight API service (e.g., Flask/FastAPI)
  - **Purpose**: Receive triggers and orchestrate calls to the `crawl4ai` service/container, returning results (REQ-FR-INT-1, REQ-FR-INT-2).
  - **Chosen because**: Leverages existing tools (N8N) or provides a dedicated, minimal service for webhook logic, keeping the core `crawl4ai` usage focused. Requires custom scripting/configuration.
- **Interface**: Docker command execution (e.g., `docker exec` or `docker run`) or a simple custom API endpoint exposed by the webhook handling service.

### Documentation
- **Format**: Markdown (.md)
  - **Purpose**: Create setup, configuration, and usage guides (REQ-FR-DOC-1, REQ-FR-DOC-2, REQ-FR-DOC-3).
  - **Chosen because**: Simple, widely used, suitable for technical documentation, integrates well with Git.

## Integration/Alignment Notes
- The core deployment involves running `crawl4ai` (likely via a Python script) within a Docker container.
- A separate PostgreSQL container will handle database storage, linked via Docker networking.
- Persistent data (configs, exported files, DB data) will use Docker volumes.
- Webhook integration requires an intermediary (like N8N or a custom Flask/FastAPI service) to trigger the `crawl4ai` container/script and handle results.
- Access control relies primarily on secure SSH access to the host DigitalOcean droplet.
- Configuration files (YAML/JSON) and documentation (Markdown) will be managed in the Git repository.

## Specification Rationale
- Using latest stable versions (Ubuntu LTS, Docker CE, PostgreSQL) balances feature availability with reliability.
- Specific `crawl4ai` version (v0.5.0) ensures adherence to project requirements.
- Standard file formats (YAML, JSON, CSV, Markdown) promote interoperability and ease of use.
- Docker provides a consistent environment for deployment and dependency management. 